{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Your goal is to create an **accurate representation of a user** based on their Google search history.\n",
    "\n",
    "The data is in `./search_history.json`. This contains a list of searches made by a single person over time.\n",
    "\n",
    "### What does \"accurate\" mean?\n",
    "\n",
    "**Accurate** means understanding which searches are **signal** and which are **noise**. Not every search reflects who someone is. Your job is to separate the meaningful from the incidental and build a coherent picture of this person.\n",
    "\n",
    "A strong solution might surface insights like:\n",
    "- **Fashion preferences**: What styles, brands, or aesthetics do they gravitate toward?\n",
    "- **Travel**: Where have they been? Where are they planning to go?\n",
    "- **Daily life**: What occupies their time—at work and for leisure?\n",
    "- **Life transitions**: Are they moving? Starting a new job? Planning a wedding?\n",
    "- **Location**: Where do they live?\n",
    "\n",
    "This is not an exhaustive list. The point is to go beyond surface-level keyword extraction and demonstrate that you *actually understand* this person.\n",
    "\n",
    "### What could a \"representation\" look like?\n",
    "\n",
    "There are many ways to represent a user. A few examples:\n",
    "- A **personal knowledge graph** capturing entities, relationships, and context\n",
    "- A **single embedding** that encodes the user's preferences in a vector space\n",
    "- An **LLM fine-tuned** on the user's data\n",
    "- An **agent** that uses RAG to answer questions about the user\n",
    "\n",
    "These are just starting points—come up with your own if you have a better idea. The specific representation you choose matters less than **why** you chose it and how well it captures what's meaningful about this person.\n",
    "\n",
    "### Dummy approach\n",
    "\n",
    "The following is what we consider a **dummy** approach:\n",
    "1. Embed all searches\n",
    "2. Cluster them by topic\n",
    "3. Label each cluster and call it a \"user interest\"\n",
    "\n",
    "This is mechanical. It doesn't distinguish signal from noise, doesn't capture nuance, and doesn't produce insights that feel *true* about a real person.\n",
    "\n",
    "### What makes an interesting approach?\n",
    "\n",
    "We're not looking for a \"correct\" answer, there probably isn't one. We're looking for **evidence of thinking**:\n",
    "- Why did you choose this method over alternatives?\n",
    "- What assumptions are you making, and why are they reasonable?\n",
    "- How do you handle ambiguity in the data?\n",
    "- What did you try that didn't work?\n",
    "\n",
    "**The reasoning behind your approach is as important as the solution itself.** Show your work. Explain your decisions. If you explored dead ends, include them.\n",
    "\n",
    "Make sure to include the cell output in the final commit. We will **not** execute the notebook ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Solution : User Intent modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The main goal here is to have a \"method\" to encode the crucial information about the crucial things related to the user, from a source of data. Here we have google search data. \n",
    "\n",
    "I intend to think this in a way that is scalable, robust to noise and the way it is actually used in production. Particularly for the use case of Fabric, I would think this as context engineering problem. When a user is chatting to their favourite LLM, depending on the user query/input the LLM decides to fetch context from Fabric's MCP (or similar) server, in this case we must correctly provide it in the most relevant and updated way, specific to the person. We need to get a system that does this for each user.\n",
    "\n",
    "The important thing is that this context can change with time. Things like user travel plans, favourite restaurants, fashion choices, the job they work, health information etc keep changing at various frequencies. So this makes approaches like finetuning an LLM on the user data not very efficient. Also finetuning can hide important info as it encodes the knowledge into LLM's parameters and the rest depends on the blackbox.\n",
    "\n",
    "Using the searches as documents for RAG helps to keep the knowledge intact, but the data is filled with a lot of noise so we cannot rely just on the semantic similarity of Vector search as it pollutes context. We need to filter and process aggressively even before the RAG step.\n",
    "\n",
    "Also the method should be scalable across users and should work with continuous updates as per the data from the user, in production. So any method that relies too much on thresholds, hardcoded cluster labels etc should be avoided (or at the best can be a baseline). \n",
    "\n",
    "##### **Considering all these, I would like to formalize the general solution framework as below:**\n",
    "\n",
    "Each user has a set of themes that they engage with in life. We can model these themes as latent variables as these are difficult to directly observe. At any given moment, a small number of themes are active, and these generate the observed user data at that instant. Some of these themes correspond to noise like random bursts or frequent but not useful events like constant gmail, google, weather visits etc. But the other relevant topics like Fashion/health/career etc form their own themes. We can find these themes from a static dataset collected with initial burn-in period of data (may be historical), and then use it to filter the upcoming searches, so each theme acts like a latent cluster (but learned from the data). All the original searches/data is stored with the label of the theme.  We infer the theme from the summary of the searches assigned to it. In inference, depending on the user query we do RAG to first get the themes (which should better align with the context) and get the most relevant and recent (we have the timestamp of the data) data, and then add it (or summarize it and then add it) to the context to get accurate information about the user. So the relevant deeper info about the person is \"mined\" as needed from a properly grouped data. This is the key idea.\n",
    "\n",
    "Now to fill specific details in each step above, I tried different methods in each stage, which I will explain in the sections below. Given the very tight compute and other resource constraints I have, I want to focus on the methodology, reasoning and trade-offs more than the performance of solutions here. I believe the approaches show promise and can surely be improved with adequate resources, I will aim to do this in future.\n",
    "\n",
    "Due to the limited resources like incapacitated machine, I tried on Google Colab GPU free tier from multiple accounts. I merged the code here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage -1 : Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This module is designed to **normalize raw user search events** (from Google or other sources) into a **lightweight, text-based representation** that can be fed into downstream models like embeddings or latent-variable models. Each raw event is a heterogeneous JSON containing fields like `title`, `time`, and `header` (common to all) and some other keys that are different for different dicts. Instead of using expensive methods like an LLM to summarize each search result, we rely on **simple, deterministic text processing** to produce a semantic description.\n",
    "\n",
    "---\n",
    "\n",
    "## Key steps\n",
    "\n",
    "1. **Timestamp normalization**\n",
    "\n",
    "   * Converts raw ISO-8601 strings to UTC timestamps using `parse_timestamp` or `datetime.fromisoformat`.\n",
    "   * Ensures all events can be sorted or split temporally.\n",
    "\n",
    "2. **Text normalization**\n",
    "\n",
    "   * Removes URLs from titles (`remove_urls`), lowercases, and tokenizes text (`tokenize`).\n",
    "   * Numeric-only tokens are discarded.\n",
    "   * Produces a clean, consistent sequence of words describing the search.\n",
    "\n",
    "3. **URL semantic extraction**\n",
    "\n",
    "   * Resolves Google redirect URLs (`resolve_google_redirect`) to their target destination.\n",
    "   * Extracts meaningful tokens from domain and path (`normalize_url`).\n",
    "   * Captures semantic hints from URLs without fetching content.\n",
    "\n",
    "4. **Event cleaning**\n",
    "\n",
    "   * Combines title tokens + URL tokens, removes duplicates, and produces a **single `description` string** per event.\n",
    "   * Preserves timestamp and event type for downstream use.\n",
    "\n",
    "5. **Document-frequency filtering**\n",
    "\n",
    "   * Computes token frequency across the corpus (`compute_document_frequency`).\n",
    "   * Removes overly common tokens (`df_filter_sentence`) that are uninformative, keeping descriptions more discriminative.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions and rationale\n",
    "\n",
    "* **Heterogeneous raw events** can be mapped to a uniform textual representation using only metadata (title + URL).\n",
    "* **Full semantic summarization is expensive**, so we rely on tokenization and URL parsing instead of LLMs.\n",
    "* **Frequent/common tokens are less informative**, so DF filtering helps models focus on distinguishing words.\n",
    "* Output descriptions are compact, deterministic, and suitable for **embedding models, clustering, or latent-variable models**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.prep import (\n",
    "    clean_events,\n",
    "    compute_document_frequency,\n",
    "    df_filter_sentence,\n",
    ")\n",
    "import json\n",
    "\n",
    "# Create artifacts directory    \n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Read the search history as a list of dicts\n",
    "with open(\"search_history.json\", \"r\") as f:\n",
    "    search_history = json.load(f)\n",
    "\n",
    "# Clean raw events \n",
    "## Each event is a dict with mandatory keys: \"timestamp\" (datetime), \"header\" (str) and \"title\" (str). But they can have other keys as well heterogeneously.\n",
    "## We need to clean them into a uniform format with keys: \"timestamp\" (datetime) and \"description\" (str). \n",
    "## Description is formed by basic cleaning from the event dict. It could be a list of keywords, removing the most used domain names and stopwords. and URLs\n",
    "\n",
    "events = clean_events(search_history)\n",
    "events = sorted(events, key=lambda x: x[\"timestamp\"])\n",
    "\n",
    "# Build corpus\n",
    "docs = [e[\"description\"] for e in events]\n",
    "df = compute_document_frequency(docs)\n",
    "\n",
    "# DF filtering\n",
    "cleaned_events = [\n",
    "    {\n",
    "        \"timestamp\": e[\"timestamp\"].timestamp(),\n",
    "        \"description\": df_filter_sentence(e[\"description\"], df, len(docs)),\n",
    "    }\n",
    "    for e in events\n",
    "]\n",
    "\n",
    "# Save artifacts from preprocessing step\n",
    "with open(\"artifacts/events.json\", \"w\") as f:\n",
    "    json.dump(events, f, default=str)\n",
    "\n",
    "with open(\"artifacts/cleaned_events.json\", \"w\") as f:\n",
    "    json.dump(cleaned_events, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "This is similar to the \"dummy\" model mentioned. Specifically, we do the following:\n",
    "\n",
    "1) Embed the cleaned description of each event using a Sentence Transformer (all Mini L6)\n",
    "2) Divide into train/val/test in the order of time, in the ratio 70, 15, 15.\n",
    "3) On the train set, start from the first event, and loop over all events. Find the dot product similarity of each event with the previous ones, and if it is similar to any one of the previous events, then they form a cluster. Else the event forms its own cluster.\n",
    "4) Keep accumulating the events into clusters and find the running stats.\n",
    "5) At the end, each cluster is considered as a \"theme\"\n",
    "\n",
    "This is done mainly for comparision with other approaches. Obviously this is very superficial, relies mostly on the LLM embeddings quality and doesn't have the notion of any user profiling task objective. Also the number of clusters grows with time, almost linearly, which is not what happens usually with humans. \n",
    "\n",
    "We compare other approaches with this to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline.model import BaselineUserModel\n",
    "from baseline.vis_helpers import visualize_theme_timeline\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create and load events\n",
    "model = BaselineUserModel()\n",
    "model.load_events(cleaned_events)\n",
    "\n",
    "# Train, val, test split respecting the time\n",
    "train, val, test = model.time_split()\n",
    "\n",
    "# Train the model\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the baseline themes\n",
    "visualize_theme_timeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained baseline model\n",
    "with open(ARTIFACTS_DIR / \"baseline_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we move closer to our initial idea of modelling latent intents. This approach didn't work out, as I explained later. But all the code related to this can be found in transformers folder in this repo. \n",
    "\n",
    "## High-level goal\n",
    "\n",
    "This model was designed to infer **latent “threads” of user intent** from a sequence of search embeddings, while explicitly modeling **time dynamics** and **thread persistence/decay**. The intended use case is a *user intent regression* problem where intent is:\n",
    "\n",
    "* **latent** (not directly supervised),\n",
    "* **temporally structured** (intents evolve over time), but slowly\n",
    "* **multi-threaded** (multiple intents can coexist), but only very close times.\n",
    "* and **softly assigned** (probabilistic, not hard clustering). I thought this would be realistic and easy training due to continuous nature of assignments.\n",
    "\n",
    "The model tries to reconstruct the original search embeddings using a low-dimensional, time-aware latent representation, while also producing a continuous **signal score** indicating the strength or relevance of intent at each timestep.\n",
    "\n",
    "---\n",
    "\n",
    "## Core assumptions (explicit)\n",
    "\n",
    "This approach rests on the following **strong assumptions**:\n",
    "\n",
    "1. **User intent can be represented as a fixed number `K` of global latent threads**\n",
    "   Each thread corresponds to a reusable intent prototype shared across users and time.\n",
    "\n",
    "2. **At each timestep, the user state is a convex mixture over threads**\n",
    "   → enforced via a softmax over `K` latent variables.\n",
    "\n",
    "3. **Thread assignment evolves smoothly over time**\n",
    "   → captured via transformer self-attention + time embeddings.\n",
    "\n",
    "4. **Time effects are multiplicative and stationary**\n",
    "   → encoded via learnable decay/boost parameters per thread.\n",
    "\n",
    "5. **Reconstruction of embeddings is sufficient supervision**\n",
    "   → no explicit intent labels, only embedding reconstruction.\n",
    "\n",
    "6. **Search embeddings are linear in intent space**\n",
    "   → decoder is a single linear projection from thread probabilities.\n",
    "\n",
    "These assumptions are *very strong* and, as explained later, several are violated in realistic user behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow explanation (step-by-step)\n",
    "\n",
    "### 1. Inputs\n",
    "\n",
    "The model consumes:\n",
    "\n",
    "* `embeds ∈ ℝ[B, T, D]`\n",
    "  Pre-computed search/query embeddings.\n",
    "\n",
    "* `timestamps, deltas`\n",
    "  Absolute and relative time information per event.\n",
    "\n",
    "* `padding_mask`\n",
    "  For variable-length sequences.\n",
    "\n",
    "No explicit intent labels are used.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Time encoding (`MultiScaleTimeEncoder`)\n",
    "\n",
    "Time is embedded into the same `d_model` space as the transformer via multiple temporal features (e.g. absolute time, relative deltas, possibly log-scaled or periodic).\n",
    "\n",
    "**Assumption**:\n",
    "Temporal effects are *additive* and can be injected as positional bias:\n",
    "\n",
    "```python\n",
    "x = input_proj(embeds) + time_embeds\n",
    "```\n",
    "\n",
    "This assumes time affects *how* intent is expressed, not *which* intent exists — a subtle but important constraint.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transformer encoder (contextualization)\n",
    "\n",
    "The transformer processes the sequence to produce contextualized representations:\n",
    "\n",
    "* Captures:\n",
    "\n",
    "  * co-occurrence patterns,\n",
    "  * temporal continuity,\n",
    "  * local bursts of activity.\n",
    "\n",
    "However, the transformer is **not autoregressive** and **not causal**, meaning:\n",
    "\n",
    "* Future queries influence earlier representations unless masked.\n",
    "\n",
    "**Implicit assumption**:\n",
    "Intent is symmetric in time within the window — which is rarely true for real user behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Latent thread inference (variational)\n",
    "\n",
    "The `thread_head` outputs:\n",
    "\n",
    "* `mean, logvar ∈ ℝ[B, T, K]`\n",
    "\n",
    "A Gaussian latent variable is sampled during training:\n",
    "\n",
    "```python\n",
    "z = mean + ε · exp(0.5 · logvar)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "probs = softmax(z)\n",
    "```\n",
    "\n",
    "This is a **VAE-style relaxation of discrete thread assignment**.\n",
    "\n",
    "**Assumptions here**:\n",
    "\n",
    "* Gaussian noise is an appropriate uncertainty model.\n",
    "* Threads lie on a simplex.\n",
    "* KL regularization (presumably elsewhere) meaningfully shapes the latent space.\n",
    "\n",
    "In practice, this often leads to:\n",
    "\n",
    "* posterior collapse,\n",
    "* over-smooth thread assignments,\n",
    "* entropy domination (uniform softmax).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Thread dynamics (decay & boost)\n",
    "\n",
    "Each thread has learnable global parameters:\n",
    "\n",
    "* `decay_k = exp(log_decay_k)`\n",
    "* `boost_k = exp(log_boost_k)`\n",
    "\n",
    "These are intended to model:\n",
    "\n",
    "* how quickly a thread fades over time,\n",
    "* how strongly it re-activates when matched.\n",
    "\n",
    "**Critical assumption**:\n",
    "\n",
    "> Thread dynamics are *stationary*, *global*, and *independent of user context*.\n",
    "\n",
    "This is almost certainly false:\n",
    "\n",
    "* user intent decay is user-specific,\n",
    "* depends on topic,\n",
    "* and depends on external events.\n",
    "\n",
    "As a result, these parameters often either:\n",
    "\n",
    "* collapse to trivial values, or\n",
    "* get ignored by the rest of the network.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Reconstruction objective\n",
    "\n",
    "Reconstruction is performed as:\n",
    "\n",
    "```python\n",
    "recon = Linear(K → D)(probs)\n",
    "```\n",
    "\n",
    "This implies:\n",
    "\n",
    "> Search embeddings lie in the **linear span of K intent prototypes**.\n",
    "\n",
    "This is the **most brittle assumption** in the entire model.\n",
    "\n",
    "Modern sentence embeddings:\n",
    "\n",
    "* are highly nonlinear,\n",
    "* encode syntax, semantics, and discourse,\n",
    "* are not additive mixtures of “intent vectors”.\n",
    "\n",
    "Thus reconstruction loss encourages:\n",
    "\n",
    "* blurry averages,\n",
    "* loss of discriminative information,\n",
    "* or trivial collapse to mean embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Signal head\n",
    "\n",
    "A scalar `signal ∈ (0, 1)` is predicted per timestep.\n",
    "\n",
    "Intended meaning:\n",
    "\n",
    "* confidence,\n",
    "* intent salience,\n",
    "* or downstream regression signal.\n",
    "\n",
    "But:\n",
    "\n",
    "* it is weakly supervised (or unsupervised),\n",
    "* competes with reconstruction gradients,\n",
    "* and has no explicit semantic grounding.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this approach failed (root causes)\n",
    "\n",
    "### 1. **Intent is not a linear mixture problem**\n",
    "\n",
    "User intent is:\n",
    "\n",
    "* hierarchical,\n",
    "* compositional,\n",
    "* often discontinuous.\n",
    "\n",
    "Forcing it into a softmax over `K` static threads destroys structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Reconstruction is the wrong objective**\n",
    "\n",
    "Reconstructing embeddings:\n",
    "\n",
    "* rewards lexical similarity,\n",
    "* not behavioral intent,\n",
    "* encourages shortcut solutions.\n",
    "\n",
    "The model can minimize loss without learning meaningful threads.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Time modeling is too weak and too global**\n",
    "\n",
    "* Additive time embeddings are insufficient.\n",
    "* Global decay parameters ignore context.\n",
    "* No explicit state transition model exists.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Variational noise hurts discrete structure**\n",
    "\n",
    "Gaussian VAEs are poorly suited for:\n",
    "\n",
    "* categorical latent structure,\n",
    "* competition between threads,\n",
    "* sparse activation.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "* entropy domination,\n",
    "* posterior collapse,\n",
    "* unused threads.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **No grounding signal**\n",
    "\n",
    "Without:\n",
    "\n",
    "* click data,\n",
    "* task completion,\n",
    "* session boundaries,\n",
    "* or downstream labels,\n",
    "\n",
    "the latent space is **unidentifiable**.\n",
    "\n",
    "Multiple radically different thread decompositions yield the same loss.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall \n",
    "\n",
    "This model is **architecturally elegant but statistically under-constrained**.\n",
    "\n",
    "It assumes:\n",
    "\n",
    "* linearity where none exists,\n",
    "* stationarity where behavior is contextual,\n",
    "* smoothness where intent is bursty,\n",
    "* and reconstructability where abstraction is required.\n",
    "\n",
    "As a result, it tends to learn soft, meaningless mixtures, collapse threads, and overfit temporal artifacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Experts + Variational AutoEncoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely — let’s go through your **Latent Variable AE → MoE-VAE pipeline** and explain **what it does, the assumptions it makes, why it worked “okay-ish,” and where the limitations come from**, all in detail and without criticizing the data.\n",
    "\n",
    "---\n",
    "\n",
    "## High-level goal\n",
    "\n",
    "This workflow was designed to discover **latent structure in event embeddings**, compressing high-dimensional sentence embeddings into a **smaller latent space** where **discrete or soft clusters (“themes”)** could emerge.\n",
    "\n",
    "The intended purpose is **concept separation / theme discovery**, similar to identifying latent topics or user intent clusters, **without explicit labels**. It combines:\n",
    "\n",
    "1. **Sentence embedding pre-processing**\n",
    "   Represent each event as a dense embedding.\n",
    "2. **Stage-0 Deep AutoEncoder (AE)**\n",
    "   Compress embeddings while preserving most information.\n",
    "3. **Stage-1 MoE-VAE**\n",
    "   Learn a mixture-of-experts latent representation where each “expert” can represent a soft concept, encouraging sparsity and separation.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step workflow\n",
    "\n",
    "### 1. Temporal splitting\n",
    "\n",
    "* Events are **chronologically sorted and split** into train, validation, and test.\n",
    "* Assumption: Temporal structure is meaningful; e.g., the model is trained only on past data relative to validation/test events.\n",
    "* Effect: Avoids data leakage and ensures that latent themes reflect sequential progression.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Embedding\n",
    "\n",
    "* Uses **sentence-transformers** to encode the `description` of each event.\n",
    "* Assumption: High-dimensional embeddings capture semantic similarity, and linear combinations of embeddings can approximate “concept mixtures.”\n",
    "* Output: `(N_events, D)` embeddings ready for compression.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Stage-0 Deep AutoEncoder (AE)\n",
    "\n",
    "* Compresses embeddings to a latent vector of size `LATENT_DIM = D / COMPRESSION_FACTOR`.\n",
    "* Uses **deep, symmetric encoder/decoder with GELU + LayerNorm + dropout**.\n",
    "* Purpose:\n",
    "\n",
    "  * Reduce dimensionality to a manageable size for MoE-VAE.\n",
    "  * Remove noise and redundancy while keeping essential semantic information.\n",
    "* Assumptions:\n",
    "\n",
    "  * Event embeddings lie on a lower-dimensional manifold.\n",
    "  * Nonlinear transformations can reconstruct the original embeddings.\n",
    "* Observed effect:\n",
    "\n",
    "  * AE often learns a **smoothed, “denoised” representation** that removes minor differences.\n",
    "  * Compressing embeddings makes later latent mixture modeling more stable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. MoE-VAE\n",
    "\n",
    "* Core idea: Each latent vector is **modeled as a mixture of K “experts”**, where each expert is a Gaussian latent distribution.\n",
    "* Steps:\n",
    "\n",
    "  1. **Encoder MLP**: maps AE-compressed embeddings → hidden space.\n",
    "  2. **Router**: computes **soft selection probabilities** over `num_experts`.\n",
    "  3. **Expert parameters**: each expert has `μ` and `logvar` for its Gaussian latent.\n",
    "  4. **Top-k sampling**: selects k most likely experts per embedding.\n",
    "  5. **Decoder MLP**: reconstructs the AE latent embeddings from the weighted expert latent vectors.\n",
    "* Loss components:\n",
    "\n",
    "  * Reconstruction (MSE)\n",
    "  * KL divergence (latent Gaussian regularization)\n",
    "  * Router entropy (sparsity encouragement)\n",
    "  * Load balancing (ensure all experts are used)\n",
    "* Assumptions:\n",
    "\n",
    "  * The latent structure is **compositional**, and events can belong partially to multiple latent themes.\n",
    "  * Sparse selection of experts encourages **interpretable separation**.\n",
    "  * Gaussian latents are sufficient to model latent variability.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Inference & cluster assignment\n",
    "\n",
    "* `infer_theme_probs` produces **soft and hard assignments** to latent experts for each event.\n",
    "* Observed effect:\n",
    "\n",
    "  * **Some concept separation emerges**: clusters correspond to different semantic themes in embeddings.\n",
    "  * Hard assignments allow downstream analysis (e.g., plotting, event grouping).\n",
    "* Limitation:\n",
    "\n",
    "  * Experts may still overlap; sparsity constraints cannot fully enforce semantic orthogonality.\n",
    "\n",
    "---\n",
    "\n",
    "## Why it worked “okay-ish”\n",
    "\n",
    "1. **AE compression helped stabilize latent modeling**\n",
    "\n",
    "   * Reduced dimensionality from ~768–1024 → 96, giving MoE-VAE a tractable input space.\n",
    "   * Removed minor embedding noise that could confuse mixture assignments.\n",
    "\n",
    "2. **MoE-VAE captures multi-modal latent structure**\n",
    "\n",
    "   * Top-k routing encourages sparsity.\n",
    "   * Load balancing prevents expert collapse (though only partially).\n",
    "   * Soft assignments enable partial membership of events to multiple latent themes, which is realistic for semantic data.\n",
    "\n",
    "3. **End-to-end latent space is semantically meaningful**\n",
    "\n",
    "   * Events with similar descriptions often cluster together.\n",
    "   * Themes are interpretable in aggregate (e.g., “finance events,” “technical updates”), even without supervision.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Reconstruction-based supervision**\n",
    "\n",
    "   * The model is trained only to reconstruct embeddings, not explicitly to maximize separability. (explicit guidance needs labels. May be a set of initial noisy labels generated by a good LLM can help here.)\n",
    "   * May produce **blurred / overlapping latent clusters**, limiting downstream utility.\n",
    "\n",
    "2. **Gaussian experts + top-k routing are approximate**\n",
    "\n",
    "   * Softmax weighting of Gaussian samples can mix distinct concepts.\n",
    "   * Variance in latent space can reduce clarity of cluster boundaries.\n",
    "\n",
    "3. **Hyperparameters matter a lot**\n",
    "\n",
    "   * Number of experts, top-k, latent size, hidden dims, AE compression all control trade-offs between separation vs. reconstruction.\n",
    "   * Small dataset + limited batch size can make sparse expert activation noisy.\n",
    "\n",
    "4. **No temporal dynamics modeled**\n",
    "\n",
    "   * Unlike the Transformer model you tried later, this pipeline ignores event timestamps beyond splitting.\n",
    "   * Could miss sequential patterns or evolving themes over time.\n",
    "\n",
    "5. **Interpretability depends on scale**\n",
    "\n",
    "   * Larger K may overfit to small differences.\n",
    "   * Smaller K may merge unrelated themes.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "* The pipeline works “okayish” because:\n",
    "\n",
    "  1. AE reduces embedding noise.\n",
    "  2. MoE-VAE captures multi-modal latent structure.\n",
    "  3. Sparse routing encourages partial concept separation.\n",
    "\n",
    "* It is limited because:\n",
    "\n",
    "  * Only reconstructs embeddings (weak supervision).\n",
    "  * Gaussian latent assumption + soft top-k may blur clusters.\n",
    "  * Temporal/sequential dependencies are ignored.\n",
    "  * Hyperparameter sensitivity and small dataset limit expert specialization.\n",
    "\n",
    "Overall, it’s a **reasonably strong unsupervised latent concept discovery method**, producing interpretable clusters better than baseline, but the performance is not still perfect. Performance could improve with better latent supervision, stronger regularization, or temporal modeling, but even as-is it does reveal some meaningful structure in your embeddings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latent_variables import config as cfg\n",
    "from latent_variables.data_split import split_by_time\n",
    "from latent_variables.embedding import embed_events\n",
    "from latent_variables.autoencoder import DeepAutoEncoder\n",
    "from latent_variables.training import train_autoencoder, encode_all\n",
    "from latent_variables.datasets import EmbeddingDataset\n",
    "from latent_variables.moe_vae import MoEVAE\n",
    "from latent_variables.losses import moe_vae_loss\n",
    "from latent_variables.inference import infer_theme_probs\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# ---- Split\n",
    "train_e, val_e, test_e = split_by_time(\n",
    "    cleaned_events, cfg.TRAIN_RATIO, cfg.VAL_RATIO\n",
    ")\n",
    "\n",
    "# ---- Embeddings\n",
    "embedder = SentenceTransformer(cfg.EMBEDDING_MODEL, device=cfg.DEVICE)\n",
    "train_emb = embed_events(train_e, embedder, cfg.EMBED_BATCH_SIZE)\n",
    "val_emb   = embed_events(val_e, embedder, cfg.EMBED_BATCH_SIZE)\n",
    "test_emb  = embed_events(test_e, embedder, cfg.EMBED_BATCH_SIZE)\n",
    "\n",
    "# ---- Stage-0 : AE\n",
    "latent_dim = train_emb.shape[1] // cfg.AE_COMPRESSION_FACTOR\n",
    "ae = DeepAutoEncoder(\n",
    "    train_emb.shape[1],\n",
    "    latent_dim,\n",
    "    cfg.AE_HIDDEN_MULTIPLIERS,\n",
    "    cfg.AE_DROPOUT\n",
    ").to(cfg.DEVICE)\n",
    "\n",
    "opt = torch.optim.AdamW(ae.parameters(), lr=cfg.AE_LR)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    EmbeddingDataset(train_emb),\n",
    "    batch_size=cfg.AE_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    EmbeddingDataset(val_emb),\n",
    "    batch_size=cfg.AE_BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_autoencoder(ae, opt, train_loader, val_loader, cfg.DEVICE, cfg.AE_EPOCHS)\n",
    "\n",
    "compressed_train = encode_all(ae, train_loader, cfg.DEVICE)\n",
    "\n",
    "# ---- Stage 1 : MoE-VAE\n",
    "model = MoEVAE(cfg).to(cfg.DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.MOE_LR)\n",
    "\n",
    "loader = DataLoader(\n",
    "    EmbeddingDataset(compressed_train),\n",
    "    batch_size=cfg.MOE_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "for epoch in range(cfg.MOE_EPOCHS):\n",
    "    model.train()\n",
    "    for x in loader:\n",
    "        x = x.to(cfg.DEVICE)\n",
    "        recon, mu, logvar, probs = model(x)\n",
    "        loss, *_ = moe_vae_loss(x, recon, mu, logvar, probs, cfg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# ---- Inference\n",
    "labels = infer_theme_probs(model, compressed_train, cfg.DEVICE, cfg.EPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained MoEVAE model\n",
    "with open(ARTIFACTS_DIR / \"moe_vae_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Resource-dependent improvements**\n",
    "\n",
    "  * Clear gains over baseline are expected with **more compute, larger datasets, and additional development time**. Due to my own constraints reg computational resources and personal reasons, I plan to dedicate more time going further on this.\n",
    "\n",
    "* **Evaluation enhancements**\n",
    "\n",
    "  * Explore **better metrics** for automated evaluation:\n",
    "\n",
    "    * LLM-based scoring/judging.\n",
    "    * Metrics derived from **behavioral studies**.\n",
    "\n",
    "* **Inference pipeline**\n",
    "\n",
    "  * Use the trained model to **group events into latent themes**.\n",
    "  * Integrate a **RAG layer** for context-aware retrieval:\n",
    "\n",
    "    * Options include **vector databases** or **graph-based RAG**.\n",
    "  * Supports **user-facing LLMs** to retrieve relevant context efficiently.\n",
    "\n",
    "* **Periodic retraining & scalability**\n",
    "\n",
    "  * Retrain the model periodically (every few months to a year) on **new user events**.\n",
    "  * Merge **old and new themes** to capture evolving patterns.\n",
    "  * Enables a **scalable, incremental update strategy** while maintaining efficiency.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
